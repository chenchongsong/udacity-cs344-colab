{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "udacity-cs344-hw5",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chenchongsong/udacity-cs344-colab/blob/main/notebook/udacity_cs344_hw5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hse6gSyUS5ka"
      },
      "cell_type": "code",
      "source": [
        "# Homework 5 for Udacity CS344 Course, Intro to Parallel Programming\n",
        "# clone the code repo,\n",
        "!git clone https://github.com/chenchongsong/udacity-cs344-colab\n",
        "!pip install git+git://github.com/depctg/nvcc4jupyter.git\n",
        "\n",
        "# load cuda plugin\n",
        "%config NVCCPluginV2.static_dir = True\n",
        "%config NVCCPluginV2.relative_dir = \"udacity-cs344-colab/src/HW5\"\n",
        "%load_ext nvcc_plugin\n",
        "\n",
        "# change to work directory, generate makefiles\n",
        "!mkdir udacity-cs344-colab/build\n",
        "%cd udacity-cs344-colab/build\n",
        "!cmake ../src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3vA0JP15TORh"
      },
      "cell_type": "code",
      "source": [
        "%%cuda --name student.cu\n",
        "\n",
        "/* Udacity HW5\n",
        "   Histogramming for Speed\n",
        "\n",
        "   The goal of this assignment is compute a histogram\n",
        "   as fast as possible.  We have simplified the problem as much as\n",
        "   possible to allow you to focus solely on the histogramming algorithm.\n",
        "\n",
        "   The input values that you need to histogram are already the exact\n",
        "   bins that need to be updated.  This is unlike in HW3 where you needed\n",
        "   to compute the range of the data and then do:\n",
        "   bin = (val - valMin) / valRange to determine the bin.\n",
        "\n",
        "   Here the bin is just:\n",
        "   bin = val\n",
        "\n",
        "   so the serial histogram calculation looks like:\n",
        "   for (i = 0; i < numElems; ++i)\n",
        "     histo[val[i]]++;\n",
        "\n",
        "   That's it!  Your job is to make it run as fast as possible!\n",
        "\n",
        "   The values are normally distributed - you may take\n",
        "   advantage of this fact in your implementation.\n",
        "\n",
        "*/\n",
        "\n",
        "\n",
        "#include \"utils.h\"\n",
        "\n",
        "#define HISTOGRAM1024_BIN_COUNT 1024\n",
        "typedef unsigned int uint;\n",
        "\n",
        "#define LOG2_WARP_SIZE 5U\n",
        "#define WARP_SIZE 32U\n",
        "\n",
        "//Warps == subhistograms per threadblock, WARP_COUNT must be <= 12 due to shared memory limit\n",
        "#define WARP_COUNT 8\n",
        "\n",
        "#define HISTOGRAM1024_THREADBLOCK_SIZE (WARP_COUNT * WARP_SIZE)\n",
        "\n",
        "//Shared memory per threadblock\n",
        "#define HISTOGRAM1024_THREADBLOCK_MEMORY (WARP_COUNT * HISTOGRAM1024_BIN_COUNT)\n",
        "\n",
        "#define UMUL(a, b) ( (a) * (b) )\n",
        "#define UMAD(a, b, c) ( UMUL((a), (b)) + (c) )\n",
        "\n",
        "inline __device__ void addWarpHist(uint *s_WarpHist, uint data) {\n",
        "    atomicAdd(s_WarpHist + data, 1);\n",
        "}\n",
        "\n",
        "__global__ void histogram1024Kernel(uint *d_PartialHistograms, uint *d_Data, uint dataCount) {\n",
        "    //Per-warp subhistogram storage\n",
        "    __shared__ uint s_Hist[HISTOGRAM1024_THREADBLOCK_MEMORY];\n",
        "    uint *s_WarpHist= s_Hist + (threadIdx.x >> LOG2_WARP_SIZE) * HISTOGRAM1024_BIN_COUNT;\n",
        "\n",
        "    //Clear shared memory storage for current threadblock before processing\n",
        "#pragma unroll\n",
        "    for (uint i = 0; i < (HISTOGRAM1024_THREADBLOCK_MEMORY / HISTOGRAM1024_THREADBLOCK_SIZE); i++) {\n",
        "        s_Hist[threadIdx.x + i * HISTOGRAM1024_THREADBLOCK_SIZE] = 0;\n",
        "    }\n",
        "    __syncthreads();\n",
        "    // till here, ~0.15ms\n",
        "\n",
        "    //Cycle through the entire data set, update subhistograms for each warp\n",
        "    for (uint pos = UMAD(blockIdx.x, blockDim.x, threadIdx.x); pos < dataCount; pos += UMUL(blockDim.x, gridDim.x)) {\n",
        "        uint data = d_Data[pos];\n",
        "        addWarpHist(s_WarpHist, data);\n",
        "    }\n",
        "    // till here, ~2.83ms\n",
        "\n",
        "    //Merge per-warp histograms into per-block and write to global memory\n",
        "    __syncthreads();\n",
        "\n",
        "    for (uint bin = threadIdx.x; bin < HISTOGRAM1024_BIN_COUNT; bin += HISTOGRAM1024_THREADBLOCK_SIZE) {\n",
        "        uint sum = 0;\n",
        "        for (uint i = 0; i < WARP_COUNT; i++) {\n",
        "            sum += s_Hist[bin + i * HISTOGRAM1024_BIN_COUNT];\n",
        "        }\n",
        "        d_PartialHistograms[blockIdx.x * HISTOGRAM1024_BIN_COUNT + bin] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////////////\n",
        "// Merge histogram1024() output\n",
        "// Run one threadblock per bin; each threadblock adds up the same bin counter\n",
        "// from every partial histogram. Reads are uncoalesced, but mergeHistogram1024\n",
        "// takes only a fraction of total processing time\n",
        "////////////////////////////////////////////////////////////////////////////////\n",
        "#define MERGE_THREADBLOCK_SIZE 256\n",
        "\n",
        "__global__ void mergeHistogram1024Kernel(\n",
        "    uint *d_Histogram,\n",
        "    uint *d_PartialHistograms,\n",
        "    uint histogramCount\n",
        ") {\n",
        "    uint sum = 0;\n",
        "\n",
        "    for (uint i = threadIdx.x; i < histogramCount; i += MERGE_THREADBLOCK_SIZE) {\n",
        "        sum += d_PartialHistograms[blockIdx.x + i * HISTOGRAM1024_BIN_COUNT];\n",
        "    }\n",
        "\n",
        "    __shared__ uint data[MERGE_THREADBLOCK_SIZE];\n",
        "    data[threadIdx.x] = sum;\n",
        "\n",
        "    // reduce\n",
        "    for (uint stride = MERGE_THREADBLOCK_SIZE / 2; stride > 0; stride >>= 1) {\n",
        "        __syncthreads();\n",
        "        if (threadIdx.x < stride) {\n",
        "            data[threadIdx.x] += data[threadIdx.x + stride];\n",
        "        }\n",
        "    }\n",
        "    if (threadIdx.x == 0) {\n",
        "        d_Histogram[blockIdx.x] = data[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////////////\n",
        "// Host interface to GPU histogram\n",
        "////////////////////////////////////////////////////////////////////////////////\n",
        "//histogram1024kernel() intermediate results buffer\n",
        "static const uint PARTIAL_HISTOGRAM1024_COUNT = 256;\n",
        "static uint *d_PartialHistograms;\n",
        "\n",
        "void computeHistogram(const uint* const d_vals, //INPUT\n",
        "                      uint* const d_histo,      //OUTPUT\n",
        "                      const uint numBins,\n",
        "                      const uint numElems) {\n",
        "  //numBins: 1024, numElems: 10240000\n",
        "\n",
        "  checkCudaErrors(cudaMalloc((void**)&d_PartialHistograms, PARTIAL_HISTOGRAM1024_COUNT * HISTOGRAM1024_BIN_COUNT * sizeof(uint)));\n",
        "  \n",
        "  // 固定thread block个数，然后每个thread block都会算出一个partial histogram\n",
        "  histogram1024Kernel<<<PARTIAL_HISTOGRAM1024_COUNT, HISTOGRAM1024_THREADBLOCK_SIZE>>>(\n",
        "      d_PartialHistograms,\n",
        "      (uint*)d_vals,\n",
        "      numElems\n",
        "  );  // ~3ms\n",
        "  checkCudaErrors(cudaGetLastError());\n",
        "\n",
        "  // 每个thread block处理所有partial histogram中的同一位\n",
        "  mergeHistogram1024Kernel<<<HISTOGRAM1024_BIN_COUNT, MERGE_THREADBLOCK_SIZE>>>(\n",
        "      d_histo,\n",
        "      d_PartialHistograms,\n",
        "      PARTIAL_HISTOGRAM1024_COUNT\n",
        "  );  // ~0.15ms\n",
        "  checkCudaErrors(cudaGetLastError());\n",
        "\n",
        "  checkCudaErrors(cudaFree(d_PartialHistograms));\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sSAnpiE2nL1T"
      },
      "cell_type": "code",
      "source": [
        "# make the cuda project\n",
        "!nvidia-smi\n",
        "!make HW5\n",
        "print(\"\\n====== RESULT OF HW5 =======\\n\")\n",
        "!bin/HW5"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}